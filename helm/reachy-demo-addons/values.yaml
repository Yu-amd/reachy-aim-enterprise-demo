aim:
  # URL-mode configuration: Direct URL to AIM endpoint (OpenAI-compatible)
  # Supports:
  #   - Cluster-internal: http://aim.default.svc.cluster.local:8000
  #   - External: https://aim.example.com
  #   - Port-forward: http://localhost:8000 (if loadgen runs locally)
  # No Service name/port assumptions - maximum flexibility
  baseUrl: "http://aim.default.svc.cluster.local:8000"
  chatPath: "/v1/chat/completions"
  model: "llm-prod"

monitoring:
  enabled: true
  installKubePromStack: false
  namespace: monitoring

dashboards:
  enabled: true
  labelKey: grafana_dashboard
  labelValue: "1"

loadgen:
  enabled: true
  schedule: "*/30 * * * *"
  image: python:3.12-slim
  durationSeconds: 60
  concurrency: 8
  qpsPerWorker: 1
  promptSet:
    - "Explain why HBM capacity matters for LLM inference in 4 sentences."
    - "Summarize what Kubernetes-native inference means."
    - "What is time-to-first-token and why does it matter?"
  apiKey:
    enabled: false
    secretName: aim-api-key
    secretKey: apiKey
