# AIM (OpenAI-compatible) Configuration
# REQUIRED: Set this to your LLM endpoint URL
# Examples:
#   - Remote AIM: http://cluster-host:8000 (via SSH port forward)
#   - Local Ollama: http://localhost:11434
#   - Local LMStudio: http://localhost:1234
#   - LMStudio on network: http://192.168.1.131:1234
AIM_BASE_URL=http://localhost:1234

# Optional AIM settings
AIM_CHAT_PATH=/v1/chat/completions
AIM_MODEL=llm-prod
AIM_API_KEY=
# Timeout for LLM requests (milliseconds). Local LLMs may need 30-60 seconds
AIM_TIMEOUT_MS=30000
AIM_MAX_RETRIES=1

# Reachy Mini Daemon Configuration
# Default port 8001 to avoid conflicts with LLM endpoints on 8000
REACHY_DAEMON_URL=http://127.0.0.1:8001
ROBOT_MODE=sim

# SLO and Metrics
E2E_SLO_MS=2500
EDGE_METRICS_HOST=127.0.0.1
EDGE_METRICS_PORT=9100
